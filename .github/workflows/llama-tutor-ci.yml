name: CI - Llama Tutor

# Se activa cuando hay cambios en llama-tutor o en el workflow mismo
on:
  push:
    branches: [main, develop]
    paths:
      - 'tutor/llama-tutor/**'
      - '.github/workflows/llama-tutor-ci.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'tutor/llama-tutor/**'
      - '.github/workflows/llama-tutor-ci.yml'

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    
    steps:
      - name: üì• Checkout c√≥digo
        uses: actions/checkout@v4

      - name: üê≥ Configurar Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üî® Build imagen Llama Tutor
        working-directory: tutor/llama-tutor
        run: |
          docker build -t llama-tutor:ci .

      - name: ü¶ô Levantar Ollama + Llama Tutor
        working-directory: tutor/llama-tutor
        run: |
          # Levantar con docker-compose
          docker compose up -d
          
          # Esperar a que Ollama descargue el modelo (puede tardar varios minutos en CI)
          echo "‚è≥ Esperando a que Ollama descargue llama3.2 (esto puede tardar 3-5 min)..."
          for i in {1..150}; do
            if docker compose exec -T ollama ollama list 2>/dev/null | grep -q llama3.2; then
              echo "‚úÖ Modelo llama3.2 descargado"
              break
            fi
            if [ $((i % 10)) -eq 0 ]; then
              echo "   Intento $i/150 ($(docker compose exec -T ollama ollama list 2>/dev/null | wc -l) modelos detectados)..."
            fi
            sleep 2
          done
          
          # Esperar a que Llama Tutor est√© listo
          echo "‚è≥ Esperando a que Llama Tutor est√© listo..."
          for i in {1..30}; do
            if curl -sf http://localhost:3001 > /dev/null 2>&1; then
              echo "‚úÖ Llama Tutor respondiendo"
              break
            fi
            echo "   Intento $i/30..."
            sleep 2
          done

      - name: üß™ Pruebas de validaci√≥n
        run: |
          # 1. Verificar que la p√°gina principal carga
          echo "üîç Test 1: P√°gina principal Llama Tutor"
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:3001)
          if [ "$HTTP_CODE" != "200" ]; then
            echo "‚ùå Fallo: p√°gina principal devolvi√≥ $HTTP_CODE"
            exit 1
          fi
          echo "‚úÖ P√°gina principal OK (HTTP 200)"
          
          # 2. Verificar que Ollama est√° respondiendo
          echo "üîç Test 2: Ollama API"
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:11434/api/tags)
          if [ "$HTTP_CODE" != "200" ]; then
            echo "‚ùå Fallo: Ollama API devolvi√≥ $HTTP_CODE"
            exit 1
          fi
          echo "‚úÖ Ollama API OK (HTTP 200)"
          
          # 3. Verificar que el modelo est√° disponible
          echo "üîç Test 3: Modelo llama3.2 disponible"
          if ! curl -s http://localhost:11434/api/tags | grep -q "llama3.2"; then
            echo "‚ùå Fallo: Modelo llama3.2 no encontrado"
            curl -s http://localhost:11434/api/tags
            exit 1
          fi
          echo "‚úÖ Modelo llama3.2 disponible"
          
          # 4. Verificar health endpoint de Llama Tutor
          echo "üîç Test 4: Health endpoint"
          HEALTH_RESPONSE=$(curl -s http://localhost:3001/api/health)
          if ! echo "$HEALTH_RESPONSE" | grep -q '"disponible":true'; then
            echo "‚ùå Fallo: Health check indica Ollama no disponible"
            echo "Respuesta: $HEALTH_RESPONSE"
            exit 1
          fi
          echo "‚úÖ Health endpoint OK - Ollama disponible"
          
          # 5. Test de chat b√°sico (puede tardar por inferencia)
          echo "üîç Test 5: Chat b√°sico con LLM (puede tardar 30-60s)"
          CHAT_RESPONSE=$(curl -s -X POST \
            -H "Content-Type: application/json" \
            -d '{
              "mensajes": [{
                "id": "test-1",
                "rol": "usuario",
                "contenido": "¬øQu√© es JavaScript?",
                "timestamp": 1234567890
              }],
              "modulo": "general"
            }' \
            http://localhost:3001/api/chat)
          
          if echo "$CHAT_RESPONSE" | grep -q '"respuesta"'; then
            RESPONSE_LENGTH=$(echo "$CHAT_RESPONSE" | jq -r '.respuesta' | wc -c)
            echo "‚úÖ Chat respondi√≥ correctamente ($RESPONSE_LENGTH chars)"
          else
            echo "‚ùå Fallo: Chat no devolvi√≥ respuesta v√°lida"
            echo "Respuesta: $CHAT_RESPONSE"
            exit 1
          fi

      - name: üìä Mostrar logs si falla
        if: failure()
        working-directory: tutor/llama-tutor
        run: |
          echo "üìã Logs de Llama Tutor:"
          docker compose logs llama-tutor
          echo ""
          echo "üìã Logs de Ollama:"
          docker compose logs ollama

      - name: üßπ Limpiar recursos
        if: always()
        working-directory: tutor/llama-tutor
        run: |
          docker compose down -v
