// M√≥dulos del curso ‚Äî mismos que copilot-tutor para consistencia
export const MODULOS = [
  { id: 'general', nombre: 'ü¶ô General', descripcion: 'Preguntas libres sobre el curso' },
  { id: 'js-fundamentos', nombre: '‚ö° JS Fundamentos', descripcion: 'let/const, arrow functions, template literals, destructuring' },
  { id: 'js-avanzado', nombre: 'üöÄ JS Avanzado', descripcion: 'Closures, async/await, event loop, prototypes, modules' },
  { id: 'react-nextjs', nombre: '‚öõÔ∏è React & Next.js', descripcion: 'Componentes, hooks, Server/Client Components, App Router' },
  { id: 'github-actions', nombre: 'üîÑ GitHub Actions', descripcion: 'Workflows, custom actions, Node.js en CI/CD' },
  { id: 'ia-cicd', nombre: 'ü§ñ IA en CI/CD', descripcion: 'LLM Gate, Ollama, Copilot CLI validators' },
] as const;

export type ModuloId = (typeof MODULOS)[number]['id'];

// Tipo para mensajes del chat
export interface Mensaje {
  id: string;
  rol: 'usuario' | 'asistente' | 'sistema';
  contenido: string;
  timestamp: number;
}

/**
 * Construye el prompt de sistema socr√°tico seg√∫n el m√≥dulo seleccionado.
 * Mismo enfoque pedag√≥gico que copilot-tutor.
 */
export function buildSystemPrompt(modulo: ModuloId): string {
  const contextoModulo: Record<ModuloId, string> = {
    'general': 'Curso completo JavaScript Cowboy (JS, React, Next.js, GitHub Actions, IA, incluye el codigo propio de los tutores)',
    'js-fundamentos': 'JavaScript ES6+ fundamentos: let/const, arrow functions, template literals, destructuring, spread/rest, clases',
    'js-avanzado': 'JavaScript avanzado: closures, async/await, promises, event loop, prototypes, modules (import/export)',
    'react-nextjs': 'React y Next.js 15: componentes, props, hooks (useState/useEffect), Server vs Client Components, App Router, Server Actions, base de datos',
    'github-actions': 'GitHub Actions con Node.js: workflows YAML, composite actions, scripts JS para CI/CD, deploy K8s',
    'ia-cicd': 'IA en CI/CD: LLM Gate con Ollama/Llama local, Copilot CLI validator, closures para clientes LLM, an√°lisis de logs con IA',
  };

  return `Eres un profesor de inform√°tica que gusta del m√©todo socr√°tico para dar explicaciones a sus alumnos del curso "JavaScript Cowboy ‚Äî De DevOps a Full-Stack con IA".
Est√°s corriendo localmente como modelo Llama v√≠a Ollama.

CONTEXTO ACTUAL: ${contextoModulo[modulo]}

REGLAS PEDAG√ìGICAS:
1. NUNCA des la respuesta directa primero. Haz una pregunta gu√≠a.
2. Usa analog√≠as de scripting linux para explicar conceptos JS/React.
3. Cuando el alumno acierte, ampl√≠a con un caso real.
4. Si se equivoca, reformula la pregunta desde otro √°ngulo.
5. Incluye snippets de c√≥digo comentados cuando ayuden.
6. Usa emojis con moderaci√≥n: ü§î para preguntas, üí° para conceptos clave, ‚ö†Ô∏è para errores comunes.
7. Responde SIEMPRE en espa√±ol.
8. Si preguntan algo fuera del curso, redirige amablemente.
9. Si es necesario para aclararar cierto concepto utiliza un diagrama ASCII (secuencia, flujo o entidades).
10. Aporta enlaces a las webs oficiales si procede.

FORMATO: Usa Markdown. C√≥digo en bloques con lenguaje. M√°ximo 700 palabras por respuesta.`;
}

// Prefijo para trazas en servidor
const TAG = '[llama]';

/**
 * Estado de conexi√≥n con Ollama ‚Äî se consulta desde el health endpoint.
 */
export interface OllamaStatus {
  disponible: boolean;
  modelo: string;
  url: string;
  error?: string;
}

/**
 * Verifica si Ollama est√° disponible y si el modelo est√° descargado.
 */
export async function verificarOllama(ollamaUrl: string, modelo: string): Promise<OllamaStatus> {
  const status: OllamaStatus = { disponible: false, modelo, url: ollamaUrl };

  try {
    console.log(`${TAG} üîç Verificando conexi√≥n con Ollama en ${ollamaUrl}...`);

    // Comprobar que Ollama responde
    const healthRes = await fetch(`${ollamaUrl}/api/tags`, { signal: AbortSignal.timeout(5000) });

    if (!healthRes.ok) {
      status.error = `Ollama respondi√≥ con ${healthRes.status}`;
      console.warn(`${TAG} ‚ö†Ô∏è  ${status.error}`);
      return status;
    }

    const { models } = await healthRes.json();
    const modelosDisponibles = (models ?? []).map((m: { name: string }) => m.name);
    console.log(`${TAG} üì¶ Modelos disponibles: ${modelosDisponibles.join(', ') || '(ninguno)'}`);

    // Verificar si el modelo solicitado est√° descargado
    const modeloPresente = modelosDisponibles.some((m: string) =>
      m === modelo || m.startsWith(modelo.split(':')[0])
    );

    if (!modeloPresente) {
      console.log(`${TAG} ‚¨áÔ∏è  Modelo "${modelo}" no encontrado. Solicitando pull...`);
      // Ollama descarga el modelo al primer uso, pero podemos pre-pull
      await fetch(`${ollamaUrl}/api/pull`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: modelo, stream: false }),
      }).catch((e) => console.warn(`${TAG} ‚ö†Ô∏è  Pull en segundo plano: ${e.message}`));
    }

    status.disponible = true;
    console.log(`${TAG} ‚úÖ Ollama disponible con modelo "${modelo}"`);
  } catch (err) {
    status.error = err instanceof Error ? err.message : 'Error desconocido';
    console.error(`${TAG} ‚ùå Ollama no disponible: ${status.error}`);
  }

  return status;
}

/**
 * Llama al servidor Ollama local para obtener una respuesta.
 * Usa la API de chat de Ollama (compatible OpenAI).
 */
export async function llamarOllama(
  mensajes: Mensaje[],
  modulo: ModuloId,
  ollamaUrl: string,
  modelo: string
): Promise<string> {
  // Construir mensajes en formato OpenAI (compatible con Ollama)
  const apiMessages = [
    { role: 'system', content: buildSystemPrompt(modulo) },
    ...mensajes
      .filter((m) => m.rol !== 'sistema')
      .map((m) => ({
        role: m.rol === 'usuario' ? 'user' : 'assistant',
        content: m.contenido,
      })),
  ];

  console.log(`${TAG} üí¨ Enviando ${apiMessages.length} mensajes a Ollama...`);
  console.log(`${TAG}    URL: ${ollamaUrl}/api/chat`);
  console.log(`${TAG}    Modelo: ${modelo}`);
  console.log(`${TAG}    √öltimo mensaje: "${apiMessages[apiMessages.length - 1].content.substring(0, 80)}..."`);

  const inicio = Date.now();

  const res = await fetch(`${ollamaUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: modelo,
      messages: apiMessages,
      stream: false,
      options: {
        temperature: 0.7,
        num_predict: 1024,
      },
    }),
  });

  const duracion = Date.now() - inicio;
  console.log(`${TAG} ‚è±Ô∏è  Respuesta en ${duracion}ms (status: ${res.status})`);

  if (!res.ok) {
    const errorBody = await res.text();
    console.error(`${TAG} ‚ùå Error Ollama: ${res.status} ‚Äî ${errorBody}`);
    throw new Error(`Ollama error ${res.status}: ${errorBody}`);
  }

  const data = await res.json();
  const respuesta = data.message?.content ?? 'Sin respuesta del modelo.';

  console.log(`${TAG} ‚úÖ Respuesta recibida (${respuesta.length} chars, ${duracion}ms)`);
  if (data.eval_count) {
    console.log(`${TAG}    Tokens evaluados: ${data.eval_count}, generados: ${data.eval_count}`);
  }

  return respuesta;
}
