name: 'ðŸ¤– LLM CI Validator (Ollama/Llama)'
description: 'Valida logs de CI/CD con LLM local (Ollama) analizando K8s, build, tests'
author: 'JavaScript Cowboy'

branding:
  icon: 'shield'
  color: 'purple'

inputs:
  context:
    description: 'JSON/TXT logs o contexto (K8s, build traces, Docker, test output)'
    required: true
  
  prompt:
    description: 'InstrucciÃ³n para el LLM (ej: "Â¿Es seguro hacer deploy con estos logs?")'
    required: true
  
  model:
    description: 'Modelo de Ollama a usar (llama3.2, codellama, mistral, etc.)'
    required: false
    default: 'llama3.2'
  
  llm-url:
    description: 'URL del servidor Ollama (localhost o self-hosted)'
    required: false
    default: 'http://localhost:11434'
  
  cache-enabled:
    description: 'Habilitar cachÃ© de respuestas LLM (reduce llamadas repetidas)'
    required: false
    default: 'true'

outputs:
  is-valid:
    description: 'ValidaciÃ³n booleana: true (CI aprobada) / false (CI bloqueada)'
    value: ${{ steps.run-validator.outputs.is-valid }}
  
  analysis:
    description: 'AnÃ¡lisis detallado del LLM sobre el contexto CI/CD'
    value: ${{ steps.run-validator.outputs.analysis }}
  
  score:
    description: 'Score numÃ©rico 0-100 de calidad/confianza del CI/CD'
    value: ${{ steps.run-validator.outputs.score }}
  
  model-used:
    description: 'Modelo LLM que generÃ³ la respuesta'
    value: ${{ steps.run-validator.outputs.model-used }}

runs:
  using: 'composite'
  steps:
    - name: ðŸŸ¢ Setup Node.js 20.x
      uses: actions/setup-node@v4
      with:
        node-version: '20.x'
        cache: 'npm'
        cache-dependency-path: ${{ github.action_path }}/src/package-lock.json

    - name: ðŸ“¦ Install Dependencies
      run: npm ci --prefix ${{ github.action_path }}/src
      shell: bash

    - name: ðŸ” Verificar Ollama Disponible
      run: |
        echo "ðŸ” Verificando conectividad con Ollama..."
        curl -f ${{ inputs.llm-url }}/api/tags || {
          echo "âŒ Error: No se puede conectar a Ollama en ${{ inputs.llm-url }}"
          echo "   AsegÃºrate de que Ollama estÃ© corriendo:"
          echo "   - Local: ollama serve"
          echo "   - Docker: docker run -d -p 11434:11434 ollama/ollama"
          exit 1
        }
        echo "âœ… Ollama estÃ¡ disponible"
      shell: bash

    - name: ðŸ¤– LLM CI Validation
      id: run-validator
      run: node ${{ github.action_path }}/src/validator.js
      shell: bash
      env:
        CONTEXT: ${{ inputs.context }}
        PROMPT: ${{ inputs.prompt }}
        MODEL: ${{ inputs.model }}
        LLM_URL: ${{ inputs.llm-url }}
        CACHE_ENABLED: ${{ inputs.cache-enabled }}
        GITHUB_OUTPUT: ${{ env.GITHUB_OUTPUT }}
        GITHUB_STEP_SUMMARY: ${{ env.GITHUB_STEP_SUMMARY }}

    - name: ðŸ“Š Generate Summary
      if: always()
      run: |
        echo "## ðŸ¤– LLM Validation Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Model**: ${{ steps.run-validator.outputs.model-used }}" >> $GITHUB_STEP_SUMMARY
        echo "**Score**: ${{ steps.run-validator.outputs.score }}/100" >> $GITHUB_STEP_SUMMARY
        echo "**Valid**: ${{ steps.run-validator.outputs.is-valid }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“ Analysis" >> $GITHUB_STEP_SUMMARY
        echo "${{ steps.run-validator.outputs.analysis }}" >> $GITHUB_STEP_SUMMARY
      shell: bash
