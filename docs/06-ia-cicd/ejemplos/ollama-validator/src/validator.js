// =============================================
// validator.js â€” LLM CI Validator con Ollama
// Ejecutado desde: action.yml
// =============================================

import fetch from 'node-fetch';
import { writeFileSync } from 'fs';

// ğŸ¯ CLOSURE: Cliente LLM con cachÃ© y estadÃ­sticas
function createLLMClient(baseUrl, cacheEnabled = true) {
  // Estado privado del closure
  const cache = new Map();
  const stats = {
    totalRequests: 0,
    cacheHits: 0,
    cacheMisses: 0,
    totalTokens: 0
  };

  return {
    async generate(model, prompt, context) {
      stats.totalRequests++;

      // ğŸ¯ Crear clave de cachÃ© Ãºnica
      const cacheKey = `${model}:${prompt.substring(0, 100)}:${context.substring(0, 100)}`;

      // ğŸ¯ Verificar cachÃ© si estÃ¡ habilitado
      if (cacheEnabled && cache.has(cacheKey)) {
        stats.cacheHits++;
        console.log(`\nğŸ“¦ CACHE HIT (${stats.cacheHits}/${stats.totalRequests})`);
        return cache.get(cacheKey);
      }

      stats.cacheMisses++;
      console.log(`\nğŸ” CACHE MISS - Llamando a LLM...`);

      // ğŸ¯ TEMPLATE LITERAL: Construir prompt completo
      const fullPrompt = `CONTEXTO CI/CD:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
${context}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

INSTRUCCIÃ“N: ${prompt}

FORMATO DE RESPUESTA REQUERIDO (JSON estricto):
{
  "isValid": true | false,
  "score": <nÃºmero 0-100>,
  "analysis": "<anÃ¡lisis detallado en espaÃ±ol>",
  "recommendations": ["<recomendaciÃ³n 1>", "<recomendaciÃ³n 2>"],
  "severity": "low" | "medium" | "high" | "critical"
}

REGLAS:
- isValid: false SI hay errores crÃ­ticos que bloqueen deploy
- score: 100 (perfecto) â†’ 0 (crÃ­tico)
- analysis: explica QUÃ‰ detectaste y POR QUÃ‰ es importante
- recommendations: pasos concretos para resolver problemas
- severity: nivel de gravedad general del contexto

RESPONDE SOLO CON EL JSON (sin markdown, sin cÃ³digo):`;

      try {
        // ğŸ¯ FETCH API: Llamar a Ollama
        const response = await fetch(`${baseUrl}/api/generate`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            model,
            prompt: fullPrompt,
            stream: false,
            options: {
              temperature: 0.3,  // MÃ¡s determinÃ­stico
              top_p: 0.9
            }
          })
        });

        if (!response.ok) {
          throw new Error(`Ollama error: ${response.status} ${response.statusText}`);
        }

        // ğŸ¯ DESTRUCTURING: Extraer respuesta
        const { response: llmResponse, total_duration, eval_count } = await response.json();

        console.log(`   âœ… Respuesta recibida (${(total_duration / 1e9).toFixed(2)}s)`);
        console.log(`   ğŸ“Š Tokens evaluados: ${eval_count || 'N/A'}`);

        if (eval_count) {
          stats.totalTokens += eval_count;
        }

        // ğŸ¯ Parsear JSON de la respuesta
        const result = extractJSON(llmResponse);

        // Guardar en cachÃ©
        if (cacheEnabled) {
          cache.set(cacheKey, result);
        }

        return result;

      } catch (error) {
        console.error(`âŒ Error en LLM request:`, error.message);
        throw error;
      }
    },

    getStats() {
      return {
        ...stats,
        cacheHitRate: stats.totalRequests > 0 
          ? ((stats.cacheHits / stats.totalRequests) * 100).toFixed(2) + '%'
          : '0%',
        cachedItems: cache.size
      };
    },

    clearCache() {
      const size = cache.size;
      cache.clear();
      console.log(`ğŸ—‘ï¸  Cache limpiado: ${size} items`);
    }
  };
}

// ğŸ¯ Extraer JSON robusto de respuesta LLM
function extractJSON(text) {
  console.log(`\nğŸ“„ Procesando respuesta LLM (${text.length} chars)...`);

  // Intentar parsear directamente
  try {
    return JSON.parse(text);
  } catch (e) {
    // Buscar bloque JSON con regex
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    
    if (jsonMatch) {
      try {
        return JSON.parse(jsonMatch[0]);
      } catch (e2) {
        console.error('âŒ JSON invÃ¡lido despuÃ©s de extracciÃ³n');
      }
    }

    // Si todo falla, construir respuesta por defecto
    console.warn('âš ï¸  No se pudo parsear JSON, usando valores por defecto');
    return {
      isValid: false,
      score: 0,
      analysis: `Error al parsear respuesta del LLM. Respuesta original: ${text.substring(0, 200)}...`,
      recommendations: ['Revisar logs del LLM', 'Verificar formato del prompt'],
      severity: 'high'
    };
  }
}

// ğŸ¯ Validar estructura de respuesta
function validateResponse(response) {
  const required = ['isValid', 'score', 'analysis'];
  
  // ğŸ¯ ARRAY.EVERY: Verificar que todos los campos existan
  const hasRequired = required.every(field => field in response);

  if (!hasRequired) {
    const missing = required.filter(field => !(field in response));
    throw new Error(`Respuesta LLM incompleta. Faltan: ${missing.join(', ')}`);
  }

  // Validar tipos
  if (typeof response.isValid !== 'boolean') {
    throw new Error('isValid debe ser boolean');
  }

  if (typeof response.score !== 'number' || response.score < 0 || response.score > 100) {
    throw new Error('score debe ser nÃºmero entre 0-100');
  }

  return true;
}

// ğŸ¯ Escribir outputs para GitHub Actions
function writeOutputs(result, model, outputFile) {
  if (!outputFile) {
    console.warn('âš ï¸  GITHUB_OUTPUT no definido');
    return;
  }

  console.log(`\nğŸ“ Escribiendo outputs a GitHub Actions...`);

  const outputs = [
    `is-valid=${result.isValid}`,
    `score=${result.score}`,
    `analysis=${result.analysis}`,
    `model-used=${model}`
  ];

  // ğŸ¯ JOIN: Unir array con saltos de lÃ­nea
  writeFileSync(outputFile, outputs.join('\n') + '\n', { flag: 'a' });

  console.log(`   âœ… Outputs escritos:`);
  outputs.forEach(output => {
    const [key] = output.split('=');
    console.log(`      - ${key}`);
  });
}

// ğŸ¯ Generar resumen visual
function generateSummary(result, stats) {
  console.log('\n' + 'â•'.repeat(60));
  console.log('ğŸ¤– LLM VALIDATION RESULT');
  console.log('â•'.repeat(60));

  // ğŸ¯ Icono segÃºn score
  const icon = result.score >= 80 ? 'âœ…' 
    : result.score >= 50 ? 'âš ï¸' 
    : 'âŒ';

  console.log(`\n${icon} ValidaciÃ³n: ${result.isValid ? 'APROBADA âœ…' : 'BLOQUEADA âŒ'}`);
  console.log(`ğŸ“Š Score: ${result.score}/100`);
  console.log(`ğŸš¨ Severity: ${result.severity || 'N/A'}`);
  
  console.log(`\nğŸ“ AnÃ¡lisis:`);
  console.log(`   ${result.analysis}`);

  if (result.recommendations?.length > 0) {
    console.log(`\nğŸ’¡ Recomendaciones:`);
    // ğŸ¯ FOREACH con Ã­ndice
    result.recommendations.forEach((rec, i) => {
      console.log(`   ${i + 1}. ${rec}`);
    });
  }

  console.log(`\nğŸ“Š EstadÃ­sticas LLM:`);
  console.log(`   Total requests: ${stats.totalRequests}`);
  console.log(`   Cache hit rate: ${stats.cacheHitRate}`);
  console.log(`   Total tokens: ${stats.totalTokens}`);

  console.log('\n' + 'â•'.repeat(60));
}

// ğŸ¯ Main function
async function main() {
  console.log('â•'.repeat(60));
  console.log('ğŸ¤– OLLAMA LLM CI VALIDATOR');
  console.log('â•'.repeat(60));

  // ğŸ¯ DESTRUCTURING con defaults
  const {
    CONTEXT,
    PROMPT,
    MODEL = 'llama3.2',
    LLM_URL = 'http://localhost:11434',
    CACHE_ENABLED = 'true',
    GITHUB_OUTPUT,
    GITHUB_STEP_SUMMARY
  } = process.env;

  // ValidaciÃ³n de inputs
  if (!CONTEXT) {
    console.error('âŒ Error: CONTEXT env var es requerida');
    process.exit(1);
  }

  if (!PROMPT) {
    console.error('âŒ Error: PROMPT env var es requerida');
    process.exit(1);
  }

  console.log(`\nâš™ï¸  ConfiguraciÃ³n:`);
  console.log(`   LLM URL: ${LLM_URL}`);
  console.log(`   Model: ${MODEL}`);
  console.log(`   Cache: ${CACHE_ENABLED === 'true' ? 'Enabled âœ…' : 'Disabled âŒ'}`);
  console.log(`   Context length: ${CONTEXT.length} chars`);
  console.log(`   Prompt: "${PROMPT.substring(0, 80)}..."`);

  try {
    // ğŸ¯ Crear cliente LLM (closure)
    const llmClient = createLLMClient(LLM_URL, CACHE_ENABLED === 'true');

    // ğŸ¯ Generar validaciÃ³n
    const result = await llmClient.generate(MODEL, PROMPT, CONTEXT);

    // ğŸ¯ Validar estructura de respuesta
    validateResponse(result);

    // ğŸ¯ Obtener estadÃ­sticas del closure
    const stats = llmClient.getStats();

    // ğŸ¯ Mostrar resumen
    generateSummary(result, stats);

    // ğŸ¯ Escribir outputs para GitHub Actions
    if (GITHUB_OUTPUT) {
      writeOutputs(result, MODEL, GITHUB_OUTPUT);
    }

    // ğŸ¯ Escribir Step Summary (markdown)
    if (GITHUB_STEP_SUMMARY) {
      const severity = result.severity || 'unknown';
      const emoji = {
        low: 'ğŸŸ¢',
        medium: 'ğŸŸ¡',
        high: 'ğŸŸ ',
        critical: 'ğŸ”´'
      }[severity] || 'âšª';

      const summary = `
## ğŸ¤– LLM Validation Result

| Metric | Value |
|--------|-------|
| **Status** | ${result.isValid ? 'âœ… Valid' : 'âŒ Invalid'} |
| **Score** | ${result.score}/100 |
| **Severity** | ${emoji} ${severity.toUpperCase()} |
| **Model** | ${MODEL} |

### ğŸ“ Analysis

${result.analysis}

${result.recommendations?.length > 0 ? `
### ğŸ’¡ Recommendations

${result.recommendations.map((r, i) => `${i + 1}. ${r}`).join('\n')}
` : ''}

### ğŸ“Š LLM Stats

- **Total Requests**: ${stats.totalRequests}
- **Cache Hit Rate**: ${stats.cacheHitRate}
- **Total Tokens**: ${stats.totalTokens}
`;

      writeFileSync(GITHUB_STEP_SUMMARY, summary, { flag: 'a' });
      console.log('\nâœ… Step Summary generado');
    }

    // ğŸ¯ Exit code segÃºn resultado
    if (!result.isValid) {
      console.log('\nâŒ CI BLOQUEADA - ValidaciÃ³n fallÃ³');
      process.exit(1);
    }

    console.log('\nâœ… CI APROBADA - ValidaciÃ³n exitosa');
    process.exit(0);

  } catch (error) {
    console.error('\nâŒ Error crÃ­tico:', error.message);
    console.error(error.stack);

    // Escribir error a outputs
    if (GITHUB_OUTPUT) {
      writeFileSync(GITHUB_OUTPUT, 
        `is-valid=false\nscore=0\nanalysis=Error: ${error.message}\nmodel-used=${MODEL}\n`,
        { flag: 'a' }
      );
    }

    process.exit(1);
  }
}

// Ejecutar
main().catch(error => {
  console.error('Fatal error:', error);
  process.exit(1);
});

// ğŸ” CONCEPTOS JAVASCRIPT USADOS:
//
// âœ… CLOSURES
//    - createLLMClient() con cache y stats privados
//    - Estado persistente entre llamadas
//    - MÃ©todos que acceden al closure
//
// âœ… ASYNC/AWAIT
//    - async function main()
//    - await llmClient.generate()
//    - await fetch() para API calls
//
// âœ… FETCH API
//    - fetch() con POST y headers
//    - await response.json()
//    - Error handling con response.ok
//
// âœ… DESTRUCTURING
//    - const { CONTEXT, PROMPT, MODEL = 'default' } = process.env
//    - const { response: llmResponse, total_duration } = await...
//    - Valores por defecto
//
// âœ… TEMPLATE LITERALS
//    - Prompt completo multilÃ­nea
//    - `${variable}` interpolaciÃ³n
//    - ConstrucciÃ³n dinÃ¡mica
//
// âœ… MAP (estructura)
//    - cache = new Map()
//    - cache.set(), get(), has()
//    - Almacenamiento clave-valor
//
// âœ… ARRAY METHODS
//    - required.every(field => field in response)
//    - required.filter(field => ...)
//    - recommendations.forEach()
//    - recommendations.map()
//
// âœ… REGEX
//    - text.match(/\{[\s\S]*\}/)
//    - ExtracciÃ³n de JSON de texto
//
// âœ… SPREAD OPERATOR
//    - { ...stats, cacheHitRate: ... }
//    - Combinar objetos
//
// âœ… CONDITIONAL (TERNARY)
//    - icon = score >= 80 ? 'âœ…' : score >= 50 ? 'âš ï¸' : 'âŒ'
//    - LÃ³gica condicional compacta
//
// âœ… STRING METHODS
//    - text.substring(0, 100)
//    - outputs.join('\n')
//    - text.length
//
// âœ… OPTIONAL CHAINING
//    - result.recommendations?.length
//    - Acceso seguro a propiedades
