# ğŸ¤– Ejemplos IA en CI/CD

Ejemplos listos para usar que integran **LLMs (Ollama/Llama)** y **GitHub Copilot CLI** como gates inteligentes en pipelines CI/CD.

---

## ğŸ“ Estructura

```
ejemplos/
â”œâ”€â”€ README.md                           â† EstÃ¡s aquÃ­
â”‚
â”œâ”€â”€ ollama-validator/                   â† LLM local (Ollama/Llama)
â”‚   â”œâ”€â”€ action.yml                      â† Composite Action
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ validator.js                â† Cliente LLM con closures
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ k8s-pod-failed.json         â† Contexto de ejemplo (logs K8s)
â”‚       â””â”€â”€ build-errors.json           â† Logs de build con errores
â”‚
â”œâ”€â”€ copilot-validator/                  â† GitHub Copilot CLI (cloud)
â”‚   â”œâ”€â”€ action.yml                      â† Composite Action
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ validator.js                â† Cliente Copilot con exec
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ test-failures.json          â† Logs de tests fallidos
â”‚       â””â”€â”€ deployment-logs.json        â† Logs de deployment
â”‚
â””â”€â”€ scripts/                            â† Utilidades standalone
    â”œâ”€â”€ llm-analyzer.js                 â† Analizador genÃ©rico LLM
    â”œâ”€â”€ prompt-builder.js               â† Constructor de prompts
    â””â”€â”€ score-calculator.js             â† Calculador de scores CI/CD
```

---

## ğŸš€ Uso RÃ¡pido

### 1. **Ollama Validator** (LLM Local)

Requiere Ollama corriendo localmente o en un runner self-hosted.

```yaml
# En tu workflow .github/workflows/ci.yml
jobs:
  validate-with-llm:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      # Capturar logs de tu app/build/tests
      - name: Capturar logs
        id: logs
        run: |
          kubectl logs pod/mi-app > /tmp/logs.txt
          echo "content=$(cat /tmp/logs.txt | jq -Rs .)" >> $GITHUB_OUTPUT
      
      # Validar con Ollama
      - name: ğŸ¤– AI Gate
        uses: ./.github/actions/ollama-validator
        with:
          context: ${{ steps.logs.outputs.content }}
          prompt: "Â¿Hay errores crÃ­ticos que impidan el deploy?"
          model: "llama3.2"
          llm-url: "http://localhost:11434"
```

**Copiar action a tu repo**:
```bash
cp -r ejemplos/ollama-validator .github/actions/
```

---

### 2. **Copilot Validator** (GitHub Cloud)

No requiere infraestructura adicional, usa tu suscripciÃ³n de GitHub Copilot.

```yaml
jobs:
  validate-with-copilot:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Capturar logs de tests
        id: test-logs
        run: |
          npm test 2>&1 | tee /tmp/test-output.txt
          echo "content=$(cat /tmp/test-output.txt | jq -Rs .)" >> $GITHUB_OUTPUT
      
      - name: ğŸ¤– Copilot AI Gate
        uses: ./.github/actions/copilot-validator
        with:
          context: ${{ steps.test-logs.outputs.content }}
          prompt: "Analiza los tests fallidos y determina si son crÃ­ticos"
          github-token: ${{ secrets.GITHUB_TOKEN }}
```

**Copiar action a tu repo**:
```bash
cp -r ejemplos/copilot-validator .github/actions/
```

---

### 3. **Scripts Standalone**

Scripts independientes para usar fuera de GitHub Actions (CI/CD local, Jenkins, GitLab, etc.)

```bash
# Analizar logs con LLM genÃ©rico
export LLM_URL="http://localhost:11434"
export LOG_FILE="./app.log"
node scripts/llm-analyzer.js

# Construir prompt Ã³ptimo para validaciÃ³n
node scripts/prompt-builder.js \
  --type "k8s-deployment" \
  --severity "critical"

# Calcular score de CI/CD
node scripts/score-calculator.js \
  --errors 3 \
  --warnings 10 \
  --coverage 85
```

---

## ğŸ”§ Setup Inicial

### OpciÃ³n A: Ollama (LLM Local)

1. **Instalar Ollama**:
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Windows
   # Descargar desde https://ollama.com/download
   ```

2. **Descargar modelo**:
   ```bash
   ollama pull llama3.2
   # o
   ollama pull codellama
   ```

3. **Verificar**:
   ```bash
   ollama list
   curl http://localhost:11434/api/generate -d '{
     "model": "llama3.2",
     "prompt": "Hola mundo"
   }'
   ```

### OpciÃ³n B: GitHub Copilot CLI

1. **Instalar Copilot CLI**:
   ```bash
   npm install -g @github/copilot-cli
   ```

2. **Autenticarse**:
   ```bash
   # Requiere suscripciÃ³n GitHub Copilot activa
   gh auth login
   ```

3. **Verificar**:
   ```bash
   gh copilot suggest "listar archivos grandes"
   ```

---

## ğŸ’¡ Conceptos JavaScript Demostrados

### âœ… **Closures**
```javascript
// validator.js
function createLLMClient(url) {
  const cache = new Map();  // ğŸ”’ Estado privado
  
  return async function(prompt, context) {
    // Acceso al cache del closure
    if (cache.has(prompt)) return cache.get(prompt);
    // ...
  };
}
```

### âœ… **Async/Await + Fetch**
```javascript
const response = await fetch(llmUrl, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ model, prompt })
});

const result = await response.json();
```

### âœ… **Destructuring**
```javascript
const {
  CONTEXT,
  PROMPT,
  MODEL = 'llama3.2',
  LLM_URL = 'http://localhost:11434'
} = process.env;
```

### âœ… **Template Literals**
```javascript
const fullPrompt = `
CONTEXTO CI/CD:
${context}

INSTRUCCIÃ“N: ${prompt}

Responde en JSON: { isValid: boolean, score: number, analysis: string }
`;
```

### âœ… **JSON Parsing Robusto**
```javascript
function extractJSON(text) {
  // Buscar bloque JSON con regex
  const match = text.match(/\{[\s\S]*\}/);
  if (match) {
    return JSON.parse(match[0]);
  }
  throw new Error('No JSON found in response');
}
```

### âœ… **Modules**
```javascript
import fetch from 'node-fetch';
import { writeFileSync } from 'fs';
import { execSync } from 'child_process';
```

---

## ğŸ“Š Flujo Completo del AI Gate

```mermaid
sequenceDiagram
    participant Workflow as GitHub Workflow
    participant Action as AI Validator Action
    participant LLM as LLM (Ollama/Copilot)
    participant Deploy as Deploy Step

    Workflow->>Workflow: 1. Build/Test
    Workflow->>Workflow: 2. Capturar logs
    Workflow->>Action: 3. context + prompt
    
    Action->>Action: 4. Construir prompt completo
    Action->>LLM: 5. POST /api/generate
    
    LLM->>LLM: 6. Analizar contexto
    LLM-->>Action: 7. { isValid, score, analysis }
    
    Action->>Action: 8. Parsear respuesta
    Action-->>Workflow: 9. Outputs (is-valid, analysis)
    
    alt is-valid === true
        Workflow->>Deploy: âœ… Continuar deploy
    else is-valid === false
        Workflow->>Workflow: âŒ STOP + mostrar analysis
    end
```

---

## ğŸ“š Casos de Uso Reales

### 1. **Validar logs de K8s antes de deploy**
```javascript
// Detectar: OOMKilled, CrashLoopBackOff, ImagePullBackOff
const context = await kubectl('logs pod/mi-app');
const result = await llmValidator(context, 
  "Â¿El pod tiene errores que impidan el deploy a producciÃ³n?"
);
```

### 2. **Analizar cobertura de tests**
```javascript
const testOutput = execSync('npm test -- --coverage');
const result = await llmValidator(testOutput,
  "Â¿La cobertura es aceptable? MÃ­nimo 80% en archivos crÃ­ticos"
);
```

### 3. **Detectar vulnerabilidades en dependencias**
```javascript
const auditOutput = execSync('npm audit --json');
const result = await llmValidator(auditOutput,
  "Â¿Hay vulnerabilidades HIGH o CRITICAL que bloqueen el deploy?"
);
```

### 4. **Validar logs de build de Docker**
```javascript
const buildLogs = fs.readFileSync('./docker-build.log');
const result = await llmValidator(buildLogs,
  "Â¿El build de Docker tiene warnings crÃ­ticos sobre seguridad?"
);
```

---

## ğŸ” Secrets Requeridos

### Para Ollama Validator:
- **Ninguno** (LLM corre localmente)
- Opcional: `OLLAMA_URL` si usas servidor remoto

### Para Copilot Validator:
- `GITHUB_TOKEN` (automÃ¡tico en GitHub Actions)
- O `COPILOT_PAT` (Personal Access Token con permisos Copilot)

---

## ğŸ§ª Testing Local

### Probar Ollama Validator:
```bash
cd ejemplos/ollama-validator/src

# Instalar deps
npm install

# Configurar contexto de ejemplo
export CONTEXT="$(cat ../examples/k8s-pod-failed.json)"
export PROMPT="Â¿Es seguro hacer deploy?"
export MODEL="llama3.2"
export LLM_URL="http://localhost:11434"

# Ejecutar
node validator.js
```

### Probar Copilot Validator:
```bash
cd ejemplos/copilot-validator/src

npm install

export CONTEXT="$(cat ../examples/test-failures.json)"
export PROMPT="Analiza los tests fallidos"
export GITHUB_TOKEN="ghp_..."

node validator.js
```

---

## ğŸ“– Recursos

- **Ollama**: https://ollama.com/
- **Llama Models**: https://ollama.com/library
- **GitHub Copilot CLI**: https://githubnext.com/projects/copilot-cli
- **MÃ³dulo 03 (Closures)**: [../03-javascript-avanzado/03-closures.md](../../03-javascript-avanzado/03-closures.md)
- **MÃ³dulo 03 (Async/Await)**: [../03-javascript-avanzado/02-async-await-promises.md](../../03-javascript-avanzado/02-async-await-promises.md)
- **MÃ³dulo 05 (GitHub Actions)**: [../05-github-actions/README.md](../../05-github-actions/README.md)

---

## ğŸ¯ PrÃ³ximos Pasos

1. âœ… Copia un validator a `.github/actions/`
2. âœ… Configura Ollama o Copilot CLI
3. âœ… Prueba localmente con ejemplos
4. âœ… Integra en tu workflow CI/CD
5. âœ… Itera sobre los prompts para mejorar precisiÃ³n

---

**ğŸš€ Â¡Tu pipeline ahora tiene un cerebro!** ğŸ§ 
