# 01 Â· LLM CI Validator con Ollama/Llama

> ğŸ¤” *Si Ollama corre Llama localmente sin API key ni cloud, Â¿puedes usarlo dentro de una GitHub Action para validar logs antes de un deploy? Â¿CuÃ¡nto JavaScript necesitas para eso?*

**Respuesta**: Solo el JS que ya aprendiste â€” closures, async/await, fetch, modules, JSON y template literals. Nada nuevo.

---

## ğŸ“ Estructura del Action

```
.github/actions/llm-ci-validator/
â”œâ”€â”€ action.yml              â† Descriptor (inputs/outputs)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ validator.js        â† Tu JS moderno
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ k8s-pod-failed.json â† Contexto de ejemplo
â””â”€â”€ README.md
```

---

## 1. `action.yml` â€” Descriptor

```yaml
name: 'ğŸ¤– LLM CI Validator (Llama)'
description: 'Valida CI/CD con Llama AI analizando logs K8s/Java/Python'
inputs:
  context:
    description: 'JSON/TXT logs (K8s, build traces, Docker)'
    required: true
  prompt:
    description: 'InstrucciÃ³n para Llama (ej: "Valida si el deploy fallÃ³")'
    required: true
  model:
    description: 'Modelo Llama (llama3.2, codellama)'
    default: 'llama3.2'
  llm-url:
    description: 'Ollama/LocalAI endpoint'
    default: 'http://localhost:11434'
outputs:
  is-valid:
    description: 'true/false â€” CI aprobada'
    value: ${{ steps.run-validator.outputs.is-valid }}
  analysis:
    description: 'AnÃ¡lisis detallado de Llama'
    value: ${{ steps.run-validator.outputs.analysis }}
  score:
    description: '0-100 score de calidad CI'
    value: ${{ steps.run-validator.outputs.score }}
runs:
  using: 'composite'
  steps:
    - name: ğŸŸ¢ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: 20.x

    - name: ğŸ“¦ Install deps
      run: npm ci --prefix ${{ github.action_path }}/src
      shell: bash

    - name: ğŸ¤– LLM CI Validation
      id: run-validator
      run: node ${{ github.action_path }}/src/validator.js
      shell: bash
      env:
        CONTEXT: ${{ inputs.context }}
        PROMPT: ${{ inputs.prompt }}
        MODEL: ${{ inputs.model }}
        LLM_URL: ${{ inputs.llm-url }}
```

---

## 2. `src/validator.js` â€” Todo tu JS Aprendido

```javascript
// âœ… Modules
import fetch from 'node-fetch';

// âœ… CLOSURE: Cliente LLM reutilizable con cachÃ©
function createLLMClient(url) {
  const cache = new Map();  // ğŸ”’ Encapsulado por closure

  return async function(model, prompt, context) {
    const cacheKey = `${model}:${prompt.slice(0, 50)}`;

    // Cache HIT
    if (cache.has(cacheKey)) {
      console.log('âœ… LLM Cache HIT');
      return cache.get(cacheKey);
    }

    // âœ… TEMPLATE LITERAL: Prompt complejo
    const fullPrompt = `CONTEXTO CI/CD:
${context}

INSTRUCCIÃ“N: ${prompt}

Responde SOLO con JSON:
{
  "isValid": true/false,
  "score": 0-100,
  "analysis": "explicaciÃ³n detallada",
  "actions": ["lista acciones correctivas"]
}`;

    // âœ… ASYNC/AWAIT + FETCH
    const response = await fetch(`${url}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        prompt: fullPrompt,
        stream: false,
        format: 'json'
      })
    });

    // âœ… JSON parse
    const result = await response.json();
    const analysis = JSON.parse(result.response);

    cache.set(cacheKey, analysis);
    return analysis;
  };
}

// âœ… ARROW + DESTRUCTURING
const main = async () => {
  try {
    const {
      CONTEXT,
      PROMPT,
      MODEL = 'llama3.2',
      LLM_URL = 'http://localhost:11434'
    } = process.env;

    console.log('ğŸ” Analizando CI con Llama...');
    console.log('ğŸ“‹ Contexto:', CONTEXT.slice(0, 200) + '...');
    console.log('ğŸ¯ Prompt:', PROMPT);

    const llm = createLLMClient(LLM_URL);
    const analysis = await llm(MODEL, PROMPT, CONTEXT);

    // Outputs para el workflow
    console.log(`::set-output name=is-valid::${analysis.isValid}`);
    console.log(`::set-output name=analysis::${analysis.analysis}`);
    console.log(`::set-output name=score::${analysis.score}`);

    console.log('âœ…', JSON.stringify(analysis, null, 2));

    // Fail CI si Llama dice NO
    if (!analysis.isValid) {
      process.exit(1);
    }
  } catch (error) {
    console.error('âŒ LLM Error:', error.message);
    process.exit(1);
  }
};

main();
```

---

## ğŸ“Š Diagrama: Flujo de validator.js

```
process.env
  â”‚ { CONTEXT, PROMPT, MODEL, LLM_URL }
  â”‚   (destructuring)
  â–¼
createLLMClient(LLM_URL)
  â”‚  cache = new Map()  â† closure
  â”‚  return async function(model, prompt, context)
  â–¼
llm(MODEL, PROMPT, CONTEXT)
  â”‚
  â”œâ”€â”€ cache.has(key)?
  â”‚   â”œâ”€â”€ âœ… return cache.get(key)
  â”‚   â””â”€â”€ âŒ continuar â†“
  â”‚
  â–¼
fullPrompt = `CONTEXTO...\n${context}\n...`
  â”‚  (template literal)
  â–¼
fetch(`${url}/api/generate`, { body: JSON.stringify(...) })
  â”‚  (async/await + fetch + JSON)
  â–¼
analysis = JSON.parse(result.response)
  â”‚
  â”œâ”€â”€ analysis.isValid === true  â†’ exit 0 (CI pasa)
  â””â”€â”€ analysis.isValid === false â†’ exit 1 (CI falla)
```

---

## 3. Workflows de Uso

### A) Validar Logs K8s (Pod Crash)

```yaml
name: ğŸ” Validate K8s Logs con Llama
on: [push]

jobs:
  validate-logs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: ğŸ“‹ Get Pod Logs
        id: logs
        run: |
          LOGS=$(kubectl logs deployment/mi-app --tail=100 -n prod || echo "No logs")
          echo "context<<EOF" >> $GITHUB_OUTPUT
          echo "$LOGS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: ğŸ¤– Llama AI Validation
        uses: ./.github/actions/llm-ci-validator
        id: llm
        with:
          context: ${{ steps.logs.outputs.context }}
          prompt: |
            Analiza estos logs de Kubernetes. Verifica:
            1. Pod crashes (OOMKilled, CrashLoopBackOff)
            2. Problemas de recursos (memory/CPU limits)
            3. Errores de conexiÃ³n DB/services
            Responde si la CI debe continuar.

      - name: ğŸš¦ CI Gate
        if: steps.llm.outputs.is-valid != 'true'
        run: |
          echo "âŒ Llama rechazÃ³ CI:"
          echo "${{ steps.llm.outputs.analysis }}"
          exit 1
```

### B) Validar Build Java/Python

```yaml
      - name: ğŸ“¦ Get Build Logs
        id: build-logs
        run: |
          BUILD_LOGS=$(cat build.log || echo "No build log")
          echo "context<<EOF" >> $GITHUB_OUTPUT
          echo "$BUILD_LOGS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: ğŸ¤– Validate Build
        uses: ./.github/actions/llm-ci-validator
        with:
          context: ${{ steps.build-logs.outputs.context }}
          prompt: |
            Analiza estos logs de build:
            1. Errores de compilaciÃ³n
            2. Dependencias rotas
            3. Tests fallidos
            4. Memory issues en build
            Â¿Es seguro hacer deploy?
```

---

## ğŸ› ï¸ Setup Local Ollama (Pruebas)

```bash
# En tu WSL/Debian
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2
ollama serve  # http://localhost:11434

# Prueba
cd .github/actions/llm-ci-validator/src
CONTEXT='{"error":"OOMKilled","pod":"mi-app"}' \
PROMPT='Â¿Es seguro hacer deploy?' \
node validator.js
```

---

[â¬…ï¸ Volver al mÃ³dulo](README.md) Â· [Siguiente: Copilot CLI Validator â¡ï¸](02-copilot-cli-validator.md)
